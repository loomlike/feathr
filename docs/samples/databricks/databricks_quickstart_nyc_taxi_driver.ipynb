{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "384e5e16-7213-4186-9d04-09d03b155534",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Feathr Quick Start Notebook\n",
    "\n",
    "This notebook illustrates the use of Feathr Feature Store to create a model that predicts NYC Taxi fares. The dataset comes from [here](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page).\n",
    "\n",
    "The major problems Feathr solves are:\n",
    "\n",
    "1. Create, share and manage useful features from raw source data.\n",
    "2. Provide Point-in-time feature join to create training dataset to ensure no data leakage.\n",
    "3. Deploy the same feature data to online store to eliminate training and inference data skew."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    "\n",
    "Feathr has native cloud integration. First step is to provision required cloud resources if you want to use Feathr.\n",
    "\n",
    "Follow the [Feathr ARM deployment guide](https://feathr-ai.github.io/feathr/how-to-guides/azure-deployment-arm.html) to run Feathr on Azure. This allows you to quickly get started with automated deployment using Azure Resource Manager template. For more details, please refer [README.md](https://github.com/feathr-ai/feathr#%EF%B8%8F-running-feathr-on-cloud-with-a-few-simple-steps).\n",
    "\n",
    "Additionally, to run this notebook, you'll need to install `feathr` pip package. For local spark, simply run `pip install feathr` on the machine that runs this notebook. To use Databricks or Azure Synapse Analytics, please see dependency management documents:\n",
    "- [Azure Databricks dependency management](https://learn.microsoft.com/en-us/azure/databricks/libraries/)\n",
    "- [Azure Synapse Analytics dependency management](https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-azure-portal-add-libraries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Steps\n",
    "\n",
    "This tutorial demonstrates the key capabilities of Feathr, including:\n",
    "\n",
    "1. Install Feathr and necessary dependancies.\n",
    "1. Create shareable features with Feathr feature definition configs.\n",
    "1. Register the features to share across teams.\n",
    "1. Create training data using point-in-time correct feature join\n",
    "1. Train and evaluate a prediction model.\n",
    "1. Materialize feature values to the online store.\n",
    "1. Fetch feature value in real-time from online store for online scoring.\n",
    "\n",
    "The overall data flow is as follows:\n",
    "\n",
    "<img src=\"https://github.com/linkedin/feathr/blob/main/docs/images/feature_flow.png?raw=true\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Feathr and Necessary Dependancies\n",
    "\n",
    "Run the following cells if you haven't installed `feathr` package already. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "80223a02-631c-40c8-91b3-a037249ffff9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "import json\n",
    "from math import sqrt\n",
    "import os\n",
    "import requests\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "from feathr import (\n",
    "    FeathrClient,\n",
    "    # Feature data types\n",
    "    BOOLEAN, FLOAT, INT32, ValueType,\n",
    "    # Feature data sources\n",
    "    INPUT_CONTEXT, HdfsSource,\n",
    "    # Feature aggregations\n",
    "    TypedKey, WindowAggTransformation,\n",
    "    # Feature types and anchor\n",
    "    DerivedFeature, Feature, FeatureAnchor,\n",
    "    # Materialization\n",
    "    BackfillTime, MaterializationSettings, RedisSink,\n",
    "    # Offline feature computation\n",
    "    FeatureQuery, ObservationSettings,\n",
    ")\n",
    "from feathr.spark_provider.feathr_configurations import SparkExecutionConfiguration\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Shareable Features with Feathr Feature Definition Configs\n",
    "\n",
    "First, we define all the necessary resource key values for authentication. These values can also be retrieved by using [Azure Key Vault](https://azure.microsoft.com/en-us/services/key-vault/) cloud key value store.\n",
    "Please refer to [A note on using azure key vault to store credentials](https://github.com/feathr-ai/feathr/blob/41e7496b38c43af6d7f8f1de842f657b27840f6d/docs/how-to-guides/feathr-configuration-and-env.md#a-note-on-using-azure-key-vault-to-store-credentials) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOURCE_PREFIX = \"jun\"\n",
    "PROJECT_NAME = \"feathr_getting_started\"\n",
    "ROOT_PATH = \"./\"  # Could be Azure Blob File System path, abfs or wasbs too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the required credentials from Azure Key Vault\n",
    "KEY_VAULT = f\"{RESOURCE_PREFIX}kv\"\n",
    "KEY_VAULT_URI = f\"https://{KEY_VAULT}.vault.azure.net\"\n",
    "\n",
    "# TODO clean up\n",
    "# synapse_workspace_url=resource_prefix+\"syws\"\n",
    "# adls_account=resource_prefix+\"dls\"\n",
    "# adls_fs_name=resource_prefix+\"fs\"\n",
    "# purview_name=resource_prefix+\"purview\"\n",
    "\n",
    "# # Set the resource link\n",
    "# os.environ['spark_config__azure_synapse__dev_url'] = f'https://{synapse_workspace_url}.dev.azuresynapse.net'\n",
    "# os.environ['spark_config__azure_synapse__pool_name'] = 'spark31'\n",
    "# os.environ['spark_config__azure_synapse__workspace_dir'] = f'abfss://{adls_fs_name}@{adls_account}.dfs.core.windows.net/feathr_project'\n",
    "# feathr_output_path = f'abfss://{adls_fs_name}@{adls_account}.dfs.core.windo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "credential = DefaultAzureCredential(exclude_interactive_browser_credential=False)\n",
    "secret_client = SecretClient(vault_url=KEY_VAULT_URI, credential=credential)\n",
    "retrieved_secret = secret_client.get_secret('FEATHR-ONLINE-STORE-CONN').value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook on **Azure Synapse** or **Local Spark**, you'll need to set `AZURE_CLIENT_ID`, `AZURE_TENANT_ID`, `AZURE_CLIENT_SECRET`, and `REDIS_PASSWORD` environment variables.\n",
    "\n",
    "To run this notebook on **Databricks**, you'll need to set `DATABRICKS_WORKSPACE_TOKEN_VALUE` and `REDIS_PASSWORD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redis credential\n",
    "os.environ['REDIS_PASSWORD'] = retrieved_secret.split(\",\")[1].split(\"password=\", 1)[1]\n",
    "\n",
    "# To use a local spark:\n",
    "os.environ['SPARK_LOCAL_IP'] = \"127.0.0.1\"\n",
    "\n",
    "# To use a databricks cluster:\n",
    "# ctx = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "# databricks_config = {\n",
    "#     'run_name': \"FEATHR_FILL_IN\",\n",
    "#     'existing_cluster_id': ctx.tags().get('clusterId').get(),\n",
    "#     'libraries': [{'jar': \"FEATHR_FILL_IN\"}],\n",
    "#     'spark_jar_task': {\n",
    "#         'main_class_name': \"FEATHR_FILL_IN\",\n",
    "#         'parameters': [\"FEATHR_FILL_IN\"],\n",
    "#     },\n",
    "# }\n",
    "# os.environ['spark_config__spark_cluster'] = \"databricks\"\n",
    "# os.environ['spark_config__databricks__workspace_instance_url'] = \"https://\" + ctx.tags().get('browserHostName').get()\n",
    "# os.environ['spark_config__databricks__config_template'] = json.dumps(databricks_config)\n",
    "# os.environ['spark_config__databricks__work_dir'] = \"dbfs:/feathr_getting_started\"\n",
    "# os.environ['DATABRICKS_WORKSPACE_TOKEN_VALUE'] = ctx.apiToken().get()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "41d3648a-9bc9-40dc-90da-bc82b21ef9b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Configurations\n",
    "\n",
    "Feathr uses a yaml file to define configurations. Please refer to [feathr_config.yaml]( https://github.com/linkedin/feathr/blob/main/feathr_project/feathrcli/data/feathr_user_workspace/feathr_config.yaml) for the meaning of each field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8cd64e3a-376c-48e6-ba41-5197f3591d48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "yaml_config = f\"\"\"\n",
    "api_version: 1\n",
    "\n",
    "project_config:\n",
    "  project_name: {PROJECT_NAME}\n",
    "  required_environment_variables:\n",
    "    - 'REDIS_PASSWORD'\n",
    "    - 'AZURE_CLIENT_ID'\n",
    "    - 'AZURE_TENANT_ID'\n",
    "    - 'AZURE_CLIENT_SECRET'\n",
    "    \n",
    "feature_registry:\n",
    "  api_endpoint: 'https://{RESOURCE_PREFIX}webapp.azurewebsites.net/api/v1'\n",
    "  # Registry configs if use purview\n",
    "  #purview:\n",
    "    #purview_name: \"feathrazuretest3-purview1\"\n",
    "    # delimiter indicates that how the project/workspace name, feature names etc. are delimited. By default it will be '__'\n",
    "    # this is for global reference (mainly for feature sharing). For example, when we setup a project called foo, and we have an anchor called 'taxi_driver' and the feature name is called 'f_daily_trips'\n",
    "    # the feature will have a globally unique name called 'foo__taxi_driver__f_daily_trips'\n",
    "    #delimiter: \"__\"\n",
    "    # controls whether the type system will be initialized or not. Usually this is only required to be executed once.\n",
    "    #type_system_initialization: false\n",
    "\n",
    "spark_config:\n",
    "  # Currently support: 'azure_synapse', 'databricks', and 'local'\n",
    "  spark_cluster: 'local'\n",
    "  spark_result_output_parts: '1'\n",
    "\n",
    "offline_store:\n",
    "  wasb:\n",
    "    wasb_enabled: true\n",
    "\n",
    "online_store:\n",
    "  # You can skip this part if you don't have Redis and skip materialization later in this notebook.\n",
    "  redis:\n",
    "    host: '{RESOURCE_PREFIX}redis.redis.cache.windows.net'\n",
    "    port: 6380\n",
    "    ssl_enabled: true\n",
    "\"\"\"\n",
    "\n",
    "tmp = NamedTemporaryFile(mode='w', delete=False)\n",
    "with open(tmp.name, \"w\") as config_file:\n",
    "    config_file.write(yaml_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the configurations can be overwritten by environment variables with concatenation of `__` for different layers of the config file. For example, `feathr_runtime_location` for databricks config can be overwritten by setting `spark_config__databricks__feathr_runtime_location` environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3fef7f2f-df19-4f53-90a5-ff7999ed983d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Initialize Feathr client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9713a2df-c7b2-4562-88b0-b7acce3cc43a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 12:31:56.283 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - secrets__azure_key_vault__name not found in the config file.\n",
      "2022-09-28 12:31:56.298 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - offline_store__s3__s3_enabled not found in the config file.\n",
      "2022-09-28 12:31:56.301 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - offline_store__adls__adls_enabled not found in the config file.\n",
      "2022-09-28 12:31:56.308 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - offline_store__jdbc__jdbc_enabled not found in the config file.\n",
      "2022-09-28 12:31:56.311 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - offline_store__snowflake__snowflake_enabled not found in the config file.\n",
      "2022-09-28 12:31:56.320 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - spark_config__local__feathr_runtime_location not found in the config file.\n",
      "2022-09-28 12:31:56.323 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - spark_config__local__workspace not found in the config file.\n",
      "2022-09-28 12:31:56.326 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - spark_config__local__master not found in the config file.\n",
      "2022-09-28 12:31:56.339 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - secrets__azure_key_vault__name not found in the config file.\n"
     ]
    }
   ],
   "source": [
    "client = FeathrClient(config_path=tmp.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c3b64bda-d42c-4a64-b976-0fb604cf38c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Prepare the NYC taxi fare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c4ccd7b3-298a-4e5a-8eec-b7e309db393e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>VendorID</th>\n",
       "      <th>lpep_pickup_datetime</th>\n",
       "      <th>lpep_dropoff_datetime</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>...</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>ehail_fee</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>trip_type</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-04-01 00:44:02</td>\n",
       "      <td>2020-04-01 00:52:23</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.68</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-04-01 00:24:39</td>\n",
       "      <td>2020-04-01 00:33:06</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>244</td>\n",
       "      <td>247</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-04-01 00:45:06</td>\n",
       "      <td>2020-04-01 00:51:13</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>244</td>\n",
       "      <td>243</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>7.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-04-01 00:45:06</td>\n",
       "      <td>2020-04-01 01:04:39</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>244</td>\n",
       "      <td>243</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.81</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>13.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-04-01 00:00:23</td>\n",
       "      <td>2020-04-01 00:16:13</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75</td>\n",
       "      <td>169</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.79</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>22.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   trip_id  VendorID lpep_pickup_datetime lpep_dropoff_datetime  \\\n",
       "0        0       2.0  2020-04-01 00:44:02   2020-04-01 00:52:23   \n",
       "1        1       2.0  2020-04-01 00:24:39   2020-04-01 00:33:06   \n",
       "2        2       2.0  2020-04-01 00:45:06   2020-04-01 00:51:13   \n",
       "3        3       2.0  2020-04-01 00:45:06   2020-04-01 01:04:39   \n",
       "4        4       2.0  2020-04-01 00:00:23   2020-04-01 00:16:13   \n",
       "\n",
       "  store_and_fwd_flag  RatecodeID  PULocationID  DOLocationID  passenger_count  \\\n",
       "0                  N         1.0            42            41              1.0   \n",
       "1                  N         1.0           244           247              2.0   \n",
       "2                  N         1.0           244           243              3.0   \n",
       "3                  N         1.0           244           243              2.0   \n",
       "4                  N         1.0            75           169              1.0   \n",
       "\n",
       "   trip_distance  ...  extra  mta_tax  tip_amount  tolls_amount  ehail_fee  \\\n",
       "0           1.68  ...    0.5      0.5         0.0           0.0        NaN   \n",
       "1           1.94  ...    0.5      0.5         0.0           0.0        NaN   \n",
       "2           1.00  ...    0.5      0.5         0.0           0.0        NaN   \n",
       "3           2.81  ...    0.5      0.5         0.0           0.0        NaN   \n",
       "4           6.79  ...    0.5      0.5         0.0           0.0        NaN   \n",
       "\n",
       "   improvement_surcharge  total_amount  payment_type  trip_type  \\\n",
       "0                    0.3           9.3           1.0        1.0   \n",
       "1                    0.3          10.3           2.0        1.0   \n",
       "2                    0.3           7.8           2.0        1.0   \n",
       "3                    0.3          13.3           2.0        1.0   \n",
       "4                    0.3          22.3           1.0        1.0   \n",
       "\n",
       "   congestion_surcharge  \n",
       "0                   0.0  \n",
       "1                   0.0  \n",
       "2                   0.0  \n",
       "3                   0.0  \n",
       "4                   0.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_FILEPATH = ROOT_PATH + \"green_tripdata_2020-04_with_index.csv\"\n",
    "\n",
    "# Download the data file\n",
    "response = requests.get(\n",
    "    \"https://azurefeathrstorage.blob.core.windows.net/public/sample_data/green_tripdata_2020-04_with_index.csv\",\n",
    ")\n",
    "with open(DATA_FILEPATH, \"wb\") as data_file:\n",
    "    data_file.write(response.content)\n",
    "\n",
    "pdf = pd.read_csv(DATA_FILEPATH, low_memory=False)\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7430c942-64e5-4b70-b823-16ce1d1b3cee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Defining features with Feathr\n",
    "\n",
    "In Feathr, a feature is viewed as a function, mapping a key and timestamp to a feature value. For more details, please see [Feathr Feature Definition Guide](https://github.com/feathr-ai/feathr/blob/main/docs/concepts/feature-definition.md).\n",
    "\n",
    "* The feature key (a.k.a. entity id) identifies the subject of feature, e.g. a user_id or location_id.\n",
    "* The feature name is the aspect of the entity that the feature is indicating, e.g. the age of the user.\n",
    "* The feature value is the actual value of that aspect at a particular time, e.g. the value is 30 at year 2022.\n",
    "\n",
    "Note that, in some cases, a feature could be just a transformation function that has no entity key or timestamp involved, e.g. *the day of week of the request timestamp*.\n",
    "\n",
    "There are two types of features -- anchored features and derivated features:\n",
    "\n",
    "* **Anchored features**: Features that are directly extracted from sources. Could be with or without aggregation. \n",
    "* **Derived features**: Features that are computed on top of other features.\n",
    "\n",
    "#### Define anchored features\n",
    "\n",
    "A feature source is needed for anchored features that describes the raw data in which the feature values are computed from. A source value should be either `INPUT_CONTEXT` (the features that will be extracted from the observation data directly) or `feathr.source.Source` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP_COL = \"lpep_dropoff_datetime\"\n",
    "TIMESTAMP_FORMAT = \"yyyy-MM-dd HH:mm:ss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a373ecbe-a040-4cd3-9d87-0d5f4c5ba553",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We define f_trip_distance and f_trip_time_duration features separately\n",
    "# so that we can reuse them later for the derived features.\n",
    "f_trip_distance = Feature(\n",
    "    name=\"f_trip_distance\",\n",
    "    feature_type=FLOAT,\n",
    "    transform=\"trip_distance\",\n",
    ")\n",
    "f_trip_time_duration = Feature(\n",
    "    name=\"f_trip_time_duration\",\n",
    "    feature_type=FLOAT,\n",
    "    transform=\"cast_float((to_unix_timestamp(lpep_dropoff_datetime) - to_unix_timestamp(lpep_pickup_datetime)) / 60)\",\n",
    ")\n",
    "\n",
    "features = [\n",
    "    f_trip_distance,\n",
    "    f_trip_time_duration,\n",
    "    Feature(\n",
    "        name=\"f_is_long_trip_distance\",\n",
    "        feature_type=BOOLEAN,\n",
    "        transform=\"trip_distance > 30.0\",\n",
    "    ),\n",
    "    Feature(\n",
    "        name=\"f_day_of_week\",\n",
    "        feature_type=INT32,\n",
    "        transform=\"dayofweek(lpep_dropoff_datetime)\",\n",
    "    ),\n",
    "    Feature(\n",
    "        name=\"f_day_of_month\",\n",
    "        feature_type=INT32,\n",
    "        transform=\"dayofmonth(lpep_dropoff_datetime)\",\n",
    "    ),\n",
    "    Feature(\n",
    "        name=\"f_hour_of_day\",\n",
    "        feature_type=INT32,\n",
    "        transform=\"hour(lpep_dropoff_datetime)\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# After you have defined features, bring them together to build the anchor to the source.\n",
    "feature_anchor = FeatureAnchor(\n",
    "    name=\"feature_anchor\",\n",
    "    source=INPUT_CONTEXT,  # Pass through source, i.e. observation data.\n",
    "    features=features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define the source with a preprocessing python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df: DataFrame) -> DataFrame:\n",
    "    import pyspark.sql.functions as F  # TODO -- always need to import modules here...\n",
    "    df = df.withColumn(\"fare_amount_cents\", (F.col(\"fare_amount\") * 100.0).cast(\"float\"))\n",
    "    return df\n",
    "\n",
    "batch_source = HdfsSource(\n",
    "    name=\"nycTaxiBatchSource\",\n",
    "    path=DATA_FILEPATH,\n",
    "    event_timestamp_column=TIMESTAMP_COL,\n",
    "    preprocessing=preprocessing,\n",
    "    timestamp_format=TIMESTAMP_FORMAT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the features with aggregation, the supported functions are as follows:\n",
    "\n",
    "| Aggregation Function | Input Type | Description |\n",
    "| --- | --- | --- |\n",
    "|SUM, COUNT, MAX, MIN, AVG\t|Numeric|Applies the the numerical operation on the numeric inputs. |\n",
    "|MAX_POOLING, MIN_POOLING, AVG_POOLING\t| Numeric Vector | Applies the max/min/avg operation on a per entry bassis for a given a collection of numbers.|\n",
    "|LATEST| Any |Returns the latest not-null values from within the defined time window |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_key = TypedKey(\n",
    "    key_column=\"DOLocationID\",\n",
    "    key_column_type=ValueType.INT32,\n",
    "    description=\"location id in NYC\",\n",
    "    full_name=\"nyc_taxi.location_id\",\n",
    ")\n",
    "\n",
    "agg_window = \"90d\"\n",
    "\n",
    "# Anchored features with aggregations\n",
    "agg_features = [\n",
    "    Feature(\n",
    "        name=\"f_location_avg_fare\",\n",
    "        key=agg_key,\n",
    "        feature_type=FLOAT,\n",
    "        transform=WindowAggTransformation(\n",
    "            agg_expr=\"fare_amount_cents\",\n",
    "            agg_func=\"AVG\",\n",
    "            window=agg_window,\n",
    "        ),\n",
    "    ),\n",
    "    Feature(\n",
    "        name=\"f_location_max_fare\",\n",
    "        key=agg_key,\n",
    "        feature_type=FLOAT,\n",
    "        transform=WindowAggTransformation(\n",
    "            agg_expr=\"fare_amount_cents\",\n",
    "            agg_func=\"MAX\",\n",
    "            window=agg_window,\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "agg_feature_anchor = FeatureAnchor(\n",
    "    name=\"agg_feature_anchor\",\n",
    "    source=batch_source,  # External data source for feature. Typically a data table.\n",
    "    features=agg_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d2ecaca9-057e-4b36-811f-320f66f753ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Define derived features\n",
    "\n",
    "We also define a derived feature, `f_trip_time_distance`, from the anchored features `f_trip_distance` and `f_trip_time_duration` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "270fb11e-8a71-404f-9639-ad29d8e6a2c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "derived_features = [\n",
    "    DerivedFeature(\n",
    "        name=\"f_trip_time_distance\",\n",
    "        feature_type=FLOAT,\n",
    "        input_features=[\n",
    "            f_trip_distance,\n",
    "            f_trip_time_duration,\n",
    "        ],\n",
    "        transform=\"f_trip_distance / f_trip_time_duration\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ad102c45-586d-468c-85f0-9454401ef10b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Build features\n",
    "\n",
    "Finally, we build the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "91bb5ebb-87e4-470b-b8eb-1c89b351740e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client.build_features(\n",
    "    anchor_list=[feature_anchor, agg_feature_anchor],\n",
    "    derived_feature_list=derived_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Register the Features to Share Across Teams\n",
    "\n",
    "You can register your features in the centralized registry and share the corresponding project with other team members who want to consume those features and for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to call registry API, status is 403, error is {\"detail\":\"write privileges for project 137917dc-07dc-464b-9b92-f226cdbf2037 required for user 56a92404-67e9-4299-bc9d-9f1d924d9f8a\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m all_features \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mlist_registered_features(project_name\u001b[38;5;241m=\u001b[39mPROJECT_NAME)\n",
      "File \u001b[0;32m/anaconda/envs/feathr/lib/python3.10/site-packages/feathr/client.py:199\u001b[0m, in \u001b[0;36mFeathrClient.register_features\u001b[0;34m(self, from_context)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manchor_list\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mderived_feature_list\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregistry\u001b[38;5;241m.\u001b[39msave_to_feature_config_from_context(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manchor_list, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mderived_feature_list, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_workspace_dir)\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_features\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_workspace_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchor_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manchor_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mderived_feature_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mderived_feature_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease call FeathrClient.build_features() first in order to register features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/anaconda/envs/feathr/lib/python3.10/site-packages/feathr/registry/_feathr_registry_client.py:112\u001b[0m, in \u001b[0;36m_FeatureRegistry.register_features\u001b[0;34m(self, workspace_path, from_context, anchor_list, derived_feature_list)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# 1. Create Source on the registry\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# We always re-create INPUT_CONTEXT as lots of existing codes reuse the singleton in different projects\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (source\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m INPUT_CONTEXT) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(source, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_registry_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m--> 112\u001b[0m     source\u001b[38;5;241m.\u001b[39m_registry_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# 2. Create Anchor on the registry\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(anchor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_registry_id\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/anaconda/envs/feathr/lib/python3.10/site-packages/feathr/registry/_feathr_registry_client.py:156\u001b[0m, in \u001b[0;36m_FeatureRegistry._create_source\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_source\u001b[39m(\u001b[38;5;28mself\u001b[39m, s: Source) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m UUID:\n\u001b[0;32m--> 156\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/projects/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproject_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/datasources\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_to_def\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m UUID(r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mguid\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    159\u001b[0m     s\u001b[38;5;241m.\u001b[39m_registry_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mid\u001b[39m\n",
      "File \u001b[0;32m/anaconda/envs/feathr/lib/python3.10/site-packages/feathr/registry/_feathr_registry_client.py:194\u001b[0m, in \u001b[0;36m_FeatureRegistry._post\u001b[0;34m(self, path, body)\u001b[0m\n\u001b[1;32m    192\u001b[0m logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPATH: \u001b[39m\u001b[38;5;124m\"\u001b[39m, path)\n\u001b[1;32m    193\u001b[0m logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBODY: \u001b[39m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(body, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m--> 194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpath\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_auth_header\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m/anaconda/envs/feathr/lib/python3.10/site-packages/feathr/registry/_feathr_registry_client.py:381\u001b[0m, in \u001b[0;36mcheck\u001b[0;34m(r)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck\u001b[39m(r):\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m r\u001b[38;5;241m.\u001b[39mok:\n\u001b[0;32m--> 381\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to call registry API, status is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, error is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to call registry API, status is 403, error is {\"detail\":\"write privileges for project 137917dc-07dc-464b-9b92-f226cdbf2037 required for user 56a92404-67e9-4299-bc9d-9f1d924d9f8a\"}"
     ]
    }
   ],
   "source": [
    "client.register_features()\n",
    "all_features = client.list_registered_features(project_name=PROJECT_NAME)  # TODO can I get other project's features than client's?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "632d5f46-f9e2-41a8-aab7-34f75206e2aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4. Create Training Data Using Point-in-Time Correct Feature Join\n",
    "\n",
    "After the feature producers have defined the features (as described in the Feature Definition part), the feature consumers may want to consume those features. Feature consumers will use observation data to query from different feature tables using Feature Query.\n",
    "\n",
    "To create a training dataset using Feathr, one needs to provide a feature join configuration file to specify\n",
    "what features and how these features should be joined to the observation data. \n",
    "\n",
    "To learn more on this topic, please refer to [Point-in-time Correctness](https://github.com/linkedin/feathr/blob/main/docs/concepts/point-in-time-join.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FORMAT = \"avro\"  # or \"parquet\"\n",
    "offline_features_path = ROOT_PATH + f\"feathr_output.{DATA_FORMAT}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features that will be extracted directly from the observation data: ['f_trip_distance', 'f_trip_time_duration', 'f_is_long_trip_distance', 'f_day_of_week', 'f_day_of_month', 'f_hour_of_day', 'f_trip_time_distance']\n",
      "Features that will be extracted from the offline source data: ['f_location_avg_fare', 'f_location_max_fare']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO to use registered features:\n",
    "# registered_features_dict = client.get_features_from_registry(PROJECT_NAME)\n",
    "# TODO: features, derived_features, agg_features = registered_features_dict\n",
    "# maybe check -- agg_features[0].key[0].key_column\n",
    "\n",
    "feature_names = [feature.name for feature in features + derived_features]\n",
    "print(\"Features that will be extracted directly from the observation data:\", feature_names)\n",
    "\n",
    "agg_feature_names = [feature.name for feature in agg_features]\n",
    "print(\"Features that will be extracted from the offline source data:\", agg_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e438e6d8-162e-4aa3-b3b3-9d1f3b0d2b7f",
     "showTitle": false,
     "title": ""
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 09:17:35.934 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - BLOB_ACCOUNT is not set in the environment variables.\n",
      "2022-09-28 09:17:35.935 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - BLOB_KEY is not set in the environment variables.\n",
      "2022-09-28 09:17:35.936 | WARNING  | feathr.spark_provider._localspark_submission:submit_feathr_job:75 - Local Spark Mode only support basic params right now and should be used only for testing purpose.\n",
      "2022-09-28 09:17:35.937 | INFO     | feathr.spark_provider._localspark_submission:_init_args:200 - Spark job: feathr_getting_started_feathr_feature_join_job is running on local spark with master: local[*].\n",
      "2022-09-28 09:17:35.945 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:120 - Detail job stdout and stderr are in debug/feathr_getting_started_feathr_feature_join_job20220928091735/log.\n",
      "2022-09-28 09:17:35.947 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:130 - Local Spark job submit with pid: 1403766.\n",
      "2022-09-28 09:17:35.948 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:139 - 27 local spark job(s) in this Launcher, only the latest will be monitored.\n",
      "2022-09-28 09:17:35.948 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:140 - Please check auto generated spark command in debug/feathr_getting_started_feathr_feature_join_job20220928091735/command.sh and detail logs in debug/feathr_getting_started_feathr_feature_join_job20220928091735/log.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug/feathr_getting_started_feathr_feature_join_job20220928091735\n",
      "['/tmp/tmpnkk_0g88/feathr_pyspark_driver.py']\n",
      "x>>>>>>>>>>>>>>>>>>>>>>>>>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 09:18:07.993 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:183 - Spark job with pid 1403766 finished in: 32 seconds with returncode 0\n",
      "2022-09-28 09:18:08.044 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - BLOB_ACCOUNT is not set in the environment variables.\n",
      "2022-09-28 09:18:08.045 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - BLOB_KEY is not set in the environment variables.\n",
      "2022-09-28 09:18:08.045 | WARNING  | feathr.spark_provider._localspark_submission:submit_feathr_job:75 - Local Spark Mode only support basic params right now and should be used only for testing purpose.\n",
      "2022-09-28 09:18:08.046 | INFO     | feathr.spark_provider._localspark_submission:_init_args:200 - Spark job: feathr_getting_started_feathr_feature_join_job is running on local spark with master: local[*].\n",
      "2022-09-28 09:18:08.050 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:120 - Detail job stdout and stderr are in debug/feathr_getting_started_feathr_feature_join_job20220928091808/log.\n",
      "2022-09-28 09:18:08.051 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:130 - Local Spark job submit with pid: 1405931.\n",
      "2022-09-28 09:18:08.052 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:139 - 28 local spark job(s) in this Launcher, only the latest will be monitored.\n",
      "2022-09-28 09:18:08.053 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:140 - Please check auto generated spark command in debug/feathr_getting_started_feathr_feature_join_job20220928091808/command.sh and detail logs in debug/feathr_getting_started_feathr_feature_join_job20220928091808/log.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">debug/feathr_getting_started_feathr_feature_join_job20220928091808\n",
      "['/tmp/tmpnkk_0g88/feathr_pyspark_driver.py']\n",
      "x>>>>>>>>>>>>x>>>>>>>x"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 09:18:45.102 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:183 - Spark job with pid 1405931 finished in: 37 seconds with returncode 0\n",
      "2022-09-28 09:18:45.151 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - BLOB_ACCOUNT is not set in the environment variables.\n",
      "2022-09-28 09:18:45.152 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - BLOB_KEY is not set in the environment variables.\n",
      "2022-09-28 09:18:45.152 | WARNING  | feathr.spark_provider._localspark_submission:submit_feathr_job:75 - Local Spark Mode only support basic params right now and should be used only for testing purpose.\n",
      "2022-09-28 09:18:45.153 | INFO     | feathr.spark_provider._localspark_submission:_init_args:200 - Spark job: feathr_getting_started_feathr_feature_join_job is running on local spark with master: local[*].\n",
      "2022-09-28 09:18:45.157 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:120 - Detail job stdout and stderr are in debug/feathr_getting_started_feathr_feature_join_job20220928091845/log.\n",
      "2022-09-28 09:18:45.158 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:130 - Local Spark job submit with pid: 1408098.\n",
      "2022-09-28 09:18:45.159 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:139 - 29 local spark job(s) in this Launcher, only the latest will be monitored.\n",
      "2022-09-28 09:18:45.160 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:140 - Please check auto generated spark command in debug/feathr_getting_started_feathr_feature_join_job20220928091845/command.sh and detail logs in debug/feathr_getting_started_feathr_feature_join_job20220928091845/log.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug/feathr_getting_started_feathr_feature_join_job20220928091845\n",
      "['/tmp/tmpnkk_0g88/feathr_pyspark_driver.py']\n",
      "x>>>>>>>>>>>>x>>>>>>>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 09:19:17.200 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:183 - Spark job with pid 1408098 finished in: 32 seconds with returncode 0\n",
      "2022-09-28 09:19:17.249 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - BLOB_ACCOUNT is not set in the environment variables.\n",
      "2022-09-28 09:19:17.251 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - BLOB_KEY is not set in the environment variables.\n",
      "2022-09-28 09:19:17.251 | WARNING  | feathr.spark_provider._localspark_submission:submit_feathr_job:75 - Local Spark Mode only support basic params right now and should be used only for testing purpose.\n",
      "2022-09-28 09:19:17.252 | INFO     | feathr.spark_provider._localspark_submission:_init_args:200 - Spark job: feathr_getting_started_feathr_feature_join_job is running on local spark with master: local[*].\n",
      "2022-09-28 09:19:17.256 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:120 - Detail job stdout and stderr are in debug/feathr_getting_started_feathr_feature_join_job20220928091917/log.\n",
      "2022-09-28 09:19:17.257 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:130 - Local Spark job submit with pid: 1410355.\n",
      "2022-09-28 09:19:17.258 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:139 - 30 local spark job(s) in this Launcher, only the latest will be monitored.\n",
      "2022-09-28 09:19:17.259 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:140 - Please check auto generated spark command in debug/feathr_getting_started_feathr_feature_join_job20220928091917/command.sh and detail logs in debug/feathr_getting_started_feathr_feature_join_job20220928091917/log.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">debug/feathr_getting_started_feathr_feature_join_job20220928091917\n",
      "['/tmp/tmpnkk_0g88/feathr_pyspark_driver.py']\n",
      "x>>>>>>>>>>>>>>>>>>>>>>>>>x"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 09:19:54.307 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:183 - Spark job with pid 1410355 finished in: 37 seconds with returncode 0\n",
      "2022-09-28 09:19:54.357 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - BLOB_ACCOUNT is not set in the environment variables.\n",
      "2022-09-28 09:19:54.358 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - BLOB_KEY is not set in the environment variables.\n",
      "2022-09-28 09:19:54.358 | WARNING  | feathr.spark_provider._localspark_submission:submit_feathr_job:75 - Local Spark Mode only support basic params right now and should be used only for testing purpose.\n",
      "2022-09-28 09:19:54.359 | INFO     | feathr.spark_provider._localspark_submission:_init_args:200 - Spark job: feathr_getting_started_feathr_feature_join_job is running on local spark with master: local[*].\n",
      "2022-09-28 09:19:54.363 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:120 - Detail job stdout and stderr are in debug/feathr_getting_started_feathr_feature_join_job20220928091954/log.\n",
      "2022-09-28 09:19:54.364 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:130 - Local Spark job submit with pid: 1412515.\n",
      "2022-09-28 09:19:54.365 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:139 - 31 local spark job(s) in this Launcher, only the latest will be monitored.\n",
      "2022-09-28 09:19:54.366 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:140 - Please check auto generated spark command in debug/feathr_getting_started_feathr_feature_join_job20220928091954/command.sh and detail logs in debug/feathr_getting_started_feathr_feature_join_job20220928091954/log.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug/feathr_getting_started_feathr_feature_join_job20220928091954\n",
      "['/tmp/tmpnkk_0g88/feathr_pyspark_driver.py']\n",
      "x>>>>>>>>>>>>x>>>>>>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 09:20:25.399 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:183 - Spark job with pid 1412515 finished in: 31 seconds with returncode 0\n",
      "2022-09-28 09:20:25.448 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - BLOB_ACCOUNT is not set in the environment variables.\n",
      "2022-09-28 09:20:25.448 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - BLOB_KEY is not set in the environment variables.\n",
      "2022-09-28 09:20:25.449 | WARNING  | feathr.spark_provider._localspark_submission:submit_feathr_job:75 - Local Spark Mode only support basic params right now and should be used only for testing purpose.\n",
      "2022-09-28 09:20:25.450 | INFO     | feathr.spark_provider._localspark_submission:_init_args:200 - Spark job: feathr_getting_started_feathr_feature_join_job is running on local spark with master: local[*].\n",
      "2022-09-28 09:20:25.454 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:120 - Detail job stdout and stderr are in debug/feathr_getting_started_feathr_feature_join_job20220928092025/log.\n",
      "2022-09-28 09:20:25.455 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:130 - Local Spark job submit with pid: 1414701.\n",
      "2022-09-28 09:20:25.456 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:139 - 32 local spark job(s) in this Launcher, only the latest will be monitored.\n",
      "2022-09-28 09:20:25.456 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:140 - Please check auto generated spark command in debug/feathr_getting_started_feathr_feature_join_job20220928092025/command.sh and detail logs in debug/feathr_getting_started_feathr_feature_join_job20220928092025/log.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">debug/feathr_getting_started_feathr_feature_join_job20220928092025\n",
      "['/tmp/tmpnkk_0g88/feathr_pyspark_driver.py']\n",
      "x>>>>>>>>>>>>x>>>>>>>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 09:20:57.507 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:183 - Spark job with pid 1414701 finished in: 32 seconds with returncode 0\n",
      "2022-09-28 09:20:57.555 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - BLOB_ACCOUNT is not set in the environment variables.\n",
      "2022-09-28 09:20:57.556 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - BLOB_KEY is not set in the environment variables.\n",
      "2022-09-28 09:20:57.557 | WARNING  | feathr.spark_provider._localspark_submission:submit_feathr_job:75 - Local Spark Mode only support basic params right now and should be used only for testing purpose.\n",
      "2022-09-28 09:20:57.558 | INFO     | feathr.spark_provider._localspark_submission:_init_args:200 - Spark job: feathr_getting_started_feathr_feature_join_job is running on local spark with master: local[*].\n",
      "2022-09-28 09:20:57.561 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:120 - Detail job stdout and stderr are in debug/feathr_getting_started_feathr_feature_join_job20220928092057/log.\n",
      "2022-09-28 09:20:57.562 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:130 - Local Spark job submit with pid: 1416857.\n",
      "2022-09-28 09:20:57.563 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:139 - 33 local spark job(s) in this Launcher, only the latest will be monitored.\n",
      "2022-09-28 09:20:57.563 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:140 - Please check auto generated spark command in debug/feathr_getting_started_feathr_feature_join_job20220928092057/command.sh and detail logs in debug/feathr_getting_started_feathr_feature_join_job20220928092057/log.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">debug/feathr_getting_started_feathr_feature_join_job20220928092057\n",
      "['/tmp/tmpnkk_0g88/feathr_pyspark_driver.py']\n",
      "x>>>>>>>>>>>>>>>>>>>>>>>>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 09:21:28.604 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:183 - Spark job with pid 1416857 finished in: 31 seconds with returncode 0\n",
      "2022-09-28 09:21:28.653 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - BLOB_ACCOUNT is not set in the environment variables.\n",
      "2022-09-28 09:21:28.654 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - BLOB_KEY is not set in the environment variables.\n",
      "2022-09-28 09:21:28.654 | WARNING  | feathr.spark_provider._localspark_submission:submit_feathr_job:75 - Local Spark Mode only support basic params right now and should be used only for testing purpose.\n",
      "2022-09-28 09:21:28.656 | INFO     | feathr.spark_provider._localspark_submission:_init_args:200 - Spark job: feathr_getting_started_feathr_feature_join_job is running on local spark with master: local[*].\n",
      "2022-09-28 09:21:28.659 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:120 - Detail job stdout and stderr are in debug/feathr_getting_started_feathr_feature_join_job20220928092128/log.\n",
      "2022-09-28 09:21:28.660 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:130 - Local Spark job submit with pid: 1419000.\n",
      "2022-09-28 09:21:28.662 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:139 - 34 local spark job(s) in this Launcher, only the latest will be monitored.\n",
      "2022-09-28 09:21:28.663 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:140 - Please check auto generated spark command in debug/feathr_getting_started_feathr_feature_join_job20220928092128/command.sh and detail logs in debug/feathr_getting_started_feathr_feature_join_job20220928092128/log.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">debug/feathr_getting_started_feathr_feature_join_job20220928092128\n",
      "['/tmp/tmpnkk_0g88/feathr_pyspark_driver.py']\n",
      "x>>>>>>>>>>>>x>>>>>>>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 09:22:00.703 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:183 - Spark job with pid 1419000 finished in: 32 seconds with returncode 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">33.2 s ± 2.48 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "# Features that we want to request. Can use a subset of features\n",
    "query = FeatureQuery(\n",
    "    feature_list=feature_names + agg_feature_names,\n",
    "    key=agg_key,\n",
    ")\n",
    "settings = ObservationSettings(\n",
    "    observation_path=DATA_FILEPATH,\n",
    "    event_timestamp_column=TIMESTAMP_COL,\n",
    "    timestamp_format=TIMESTAMP_FORMAT,\n",
    ")\n",
    "client.get_offline_features(\n",
    "    observation_settings=settings,\n",
    "    feature_query=query,\n",
    "    # TODO - this doesn't work. It keeps storing as \"avro\" files\n",
    "    execution_configurations=SparkExecutionConfiguration({\n",
    "        \"spark.feathr.inputFormat\": DATA_FORMAT,\n",
    "        \"spark.feathr.outputFormat\": DATA_FORMAT,\n",
    "    }),\n",
    "    output_path=offline_features_path,\n",
    ")\n",
    "\n",
    "client.wait_job_to_finish(timeout_sec=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dcbf17fc-7f79-4a65-a3af-9cffbd0b5d1f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 5. Train and Evaluate a Prediction Model\n",
    "\n",
    "After generating all the features, we train and evaluate a machine learning model to predict the NYC taxi fare prediction. In this example, we use Spark MLlib's [GBTRegressor](https://spark.apache.org/docs/latest/ml-classification-regression.html#gradient-boosted-tree-regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b9be042e-eb12-46b9-9d91-a0e5dd0c704f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/anaconda/envs/feathr/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/anaconda/envs/feathr/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jumin/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jumin/.ivy2/jars\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-446439de-7e7f-4b98-b3fb-abf4dc025b46;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.3.0 in central\n",
      "\tfound org.tukaani#xz;1.8 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      ":: resolution report :: resolve 203ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.spark#spark-avro_2.12;3.3.0 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.tukaani#xz;1.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-446439de-7e7f-4b98-b3fb-abf4dc025b46\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/6ms)\n",
      "22/09/28 12:36:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "\n",
    "# To run on a local spark, start a spark session:\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"feathr\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.3.0\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Train and Test Data from the Offline Feature Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/28 12:36:35 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>VendorID</th>\n",
       "      <th>lpep_pickup_datetime</th>\n",
       "      <th>lpep_dropoff_datetime</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>...</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>f_is_long_trip_distance</th>\n",
       "      <th>f_day_of_month</th>\n",
       "      <th>f_trip_time_duration</th>\n",
       "      <th>f_day_of_week</th>\n",
       "      <th>f_hour_of_day</th>\n",
       "      <th>f_trip_distance</th>\n",
       "      <th>f_location_max_fare</th>\n",
       "      <th>f_location_avg_fare</th>\n",
       "      <th>f_trip_time_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24504</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-04-01 03:26:00</td>\n",
       "      <td>2020-04-01 03:42:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>248</td>\n",
       "      <td>137</td>\n",
       "      <td>None</td>\n",
       "      <td>10.06</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>10.06</td>\n",
       "      <td>2266.0</td>\n",
       "      <td>2266.000000</td>\n",
       "      <td>0.628750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24531</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-04-01 05:35:00</td>\n",
       "      <td>2020-04-01 06:04:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>26</td>\n",
       "      <td>137</td>\n",
       "      <td>None</td>\n",
       "      <td>10.7</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>10.70</td>\n",
       "      <td>3820.0</td>\n",
       "      <td>3043.000000</td>\n",
       "      <td>0.368966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-04-01 07:16:53</td>\n",
       "      <td>2020-04-01 07:24:52</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>74</td>\n",
       "      <td>137</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.19</td>\n",
       "      <td>...</td>\n",
       "      <td>2.75</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>7.983333</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>5.19</td>\n",
       "      <td>3820.0</td>\n",
       "      <td>2545.333252</td>\n",
       "      <td>0.650104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-04-01 07:26:49</td>\n",
       "      <td>2020-04-01 07:45:30</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>74</td>\n",
       "      <td>137</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.26</td>\n",
       "      <td>...</td>\n",
       "      <td>2.75</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>18.683332</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>5.26</td>\n",
       "      <td>3820.0</td>\n",
       "      <td>2396.500000</td>\n",
       "      <td>0.281534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>115</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-04-01 07:41:47</td>\n",
       "      <td>2020-04-01 07:58:31</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41</td>\n",
       "      <td>137</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.26</td>\n",
       "      <td>...</td>\n",
       "      <td>2.75</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>16.733334</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>6.26</td>\n",
       "      <td>3820.0</td>\n",
       "      <td>2327.199951</td>\n",
       "      <td>0.374104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  trip_id VendorID lpep_pickup_datetime lpep_dropoff_datetime  \\\n",
       "0   24504     None  2020-04-01 03:26:00   2020-04-01 03:42:00   \n",
       "1   24531     None  2020-04-01 05:35:00   2020-04-01 06:04:00   \n",
       "2     104      2.0  2020-04-01 07:16:53   2020-04-01 07:24:52   \n",
       "3      84      2.0  2020-04-01 07:26:49   2020-04-01 07:45:30   \n",
       "4     115      2.0  2020-04-01 07:41:47   2020-04-01 07:58:31   \n",
       "\n",
       "  store_and_fwd_flag RatecodeID PULocationID DOLocationID passenger_count  \\\n",
       "0               None       None          248          137            None   \n",
       "1               None       None           26          137            None   \n",
       "2                  N        1.0           74          137             2.0   \n",
       "3                  N        1.0           74          137             1.0   \n",
       "4                  N        1.0           41          137             1.0   \n",
       "\n",
       "  trip_distance  ... congestion_surcharge f_is_long_trip_distance  \\\n",
       "0         10.06  ...                 None                   False   \n",
       "1          10.7  ...                 None                   False   \n",
       "2          5.19  ...                 2.75                   False   \n",
       "3          5.26  ...                 2.75                   False   \n",
       "4          6.26  ...                 2.75                   False   \n",
       "\n",
       "  f_day_of_month f_trip_time_duration f_day_of_week f_hour_of_day  \\\n",
       "0              1            16.000000             4             3   \n",
       "1              1            29.000000             4             6   \n",
       "2              1             7.983333             4             7   \n",
       "3              1            18.683332             4             7   \n",
       "4              1            16.733334             4             7   \n",
       "\n",
       "  f_trip_distance f_location_max_fare f_location_avg_fare f_trip_time_distance  \n",
       "0           10.06              2266.0         2266.000000             0.628750  \n",
       "1           10.70              3820.0         3043.000000             0.368966  \n",
       "2            5.19              3820.0         2545.333252             0.650104  \n",
       "3            5.26              3820.0         2396.500000             0.281534  \n",
       "4            6.26              3820.0         2327.199951             0.374104  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format(DATA_FORMAT).load(offline_features_path)\n",
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / test split\n",
    "train_df, test_df = (\n",
    "    df\n",
    "    .withColumn(\"label\", F.col(\"fare_amount\").cast(\"double\"))\n",
    "    .where(F.col(\"f_trip_time_duration\") > 0)\n",
    "    .fillna(0)\n",
    "    .randomSplit([0.8, 0.2])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a feature vector column for SparkML\n",
    "vector_assembler = VectorAssembler(\n",
    "    inputCols=[x for x in df.columns if x in feature_names + agg_feature_names],\n",
    "    outputCol=\"features\",\n",
    ")\n",
    "\n",
    "# Define a model\n",
    "gbt = GBTRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    maxIter=100,\n",
    "    maxDepth=5,\n",
    "    maxBins=16,\n",
    ")\n",
    "\n",
    "# Create a ML pipeline\n",
    "ml_pipeline = Pipeline(stages=[\n",
    "    vector_assembler,\n",
    "    gbt,\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 5.58846\n"
     ]
    }
   ],
   "source": [
    "# Train a model\n",
    "model = ml_pipeline.fit(train_df)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "# Evaluate\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\",\n",
    ")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5a226026-1c7b-48db-8f91-88d5c2ddf023",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 6. Materialize Feature Values to the Online Store\n",
    "\n",
    "While we computed feature values on-the-fly at request time via Feathr, we can pre-compute the feature values and materialize them to offline or online storages such as Redis.\n",
    "\n",
    "Note, only the features anchored to offline data source can be materialized, in our case `agg_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3b924c66-8634-42fe-90f3-c844487d3f75",
     "showTitle": false,
     "title": ""
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 14:16:11.867 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - KAFKA_SASL_JAAS_CONFIG is not set in the environment variables.\n",
      "2022-09-28 14:16:11.868 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - BLOB_ACCOUNT is not set in the environment variables.\n",
      "2022-09-28 14:16:11.869 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - BLOB_KEY is not set in the environment variables.\n",
      "2022-09-28 14:16:11.872 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - monitoring__database__sql__url not found in the config file.\n",
      "2022-09-28 14:16:11.876 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - monitoring__database__sql__user not found in the config file.\n",
      "2022-09-28 14:16:11.877 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - MONITORING_DATABASE_SQL_PASSWORD is not set in the environment variables.\n",
      "2022-09-28 14:16:11.877 | WARNING  | feathr.spark_provider._localspark_submission:submit_feathr_job:75 - Local Spark Mode only support basic params right now and should be used only for testing purpose.\n",
      "2022-09-28 14:16:11.879 | INFO     | feathr.spark_provider._localspark_submission:_init_args:200 - Spark job: feathr_getting_started_feathr_feature_materialization_job is running on local spark with master: local[*].\n",
      "2022-09-28 14:16:11.882 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:120 - Detail job stdout and stderr are in debug/feathr_getting_started_feathr_feature_materialization_job20220928141611/log.\n",
      "2022-09-28 14:16:11.883 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:130 - Local Spark job submit with pid: 1497110.\n",
      "2022-09-28 14:16:11.884 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:139 - 154 local spark job(s) in this Launcher, only the latest will be monitored.\n",
      "2022-09-28 14:16:11.885 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:140 - Please check auto generated spark command in debug/feathr_getting_started_feathr_feature_materialization_job20220928141611/command.sh and detail logs in debug/feathr_getting_started_feathr_feature_materialization_job20220928141611/log.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug/feathr_getting_started_feathr_feature_materialization_job20220928141611\n",
      "['/tmp/tmppbbifv2u/feathr_pyspark_driver.py']\n",
      "x>x"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 14:16:24.897 | WARNING  | feathr.spark_provider._localspark_submission:wait_for_completion:180 - Spark job with pid 1497110 is not successful, please check.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Spark job failed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [50], line 29\u001b[0m\n\u001b[1;32m     17\u001b[0m settings \u001b[38;5;241m=\u001b[39m MaterializationSettings(\n\u001b[1;32m     18\u001b[0m     name\u001b[38;5;241m=\u001b[39mFEATURE_TABLE_NAME \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.job\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# job name -- TODO if not important, automate this, e.g. redis_sink.table_name + \".job\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     backfill_time\u001b[38;5;241m=\u001b[39mbackfill_time,\n\u001b[1;32m     20\u001b[0m     sinks\u001b[38;5;241m=\u001b[39m[redis_sink],  \u001b[38;5;66;03m# and/or adls_sink -- TODO can I sepcify both at the same time?\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     feature_names\u001b[38;5;241m=\u001b[39magg_feature_names,\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m client\u001b[38;5;241m.\u001b[39mmaterialize_features(\n\u001b[1;32m     25\u001b[0m     settings\u001b[38;5;241m=\u001b[39msettings,\n\u001b[1;32m     26\u001b[0m     execution_configurations\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.feathr.outputFormat\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     27\u001b[0m )\n\u001b[0;32m---> 29\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_job_to_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_sec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/feathr/lib/python3.10/site-packages/feathr/client.py:713\u001b[0m, in \u001b[0;36mFeathrClient.wait_job_to_finish\u001b[0;34m(self, timeout_sec)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 713\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpark job failed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Spark job failed."
     ]
    }
   ],
   "source": [
    "FEATURE_TABLE_NAME = \"nycTaxiDemoFeature\"\n",
    "\n",
    "# Time range to materialize -- TODO how to properly set this?\n",
    "backfill_time = BackfillTime(\n",
    "    start=datetime(2020, 5, 20),\n",
    "    end=datetime(2020, 5, 20),\n",
    "    step=timedelta(days=1),\n",
    ")\n",
    "\n",
    "# Destinations\n",
    "# For online store,\n",
    "redis_sink = RedisSink(table_name=FEATURE_TABLE_NAME)\n",
    "\n",
    "# For offline store,\n",
    "# adls_sink = HdfsSink(output_path=f\"abfss://{adls_fs_name}@{adls_account}.dfs.core.windows.net/{FEATURE_TABLE_NAME}/\")\n",
    "\n",
    "settings = MaterializationSettings(\n",
    "    name=FEATURE_TABLE_NAME + \".job\",  # job name -- TODO if not important, automate this, e.g. redis_sink.table_name + \".job\"\n",
    "    backfill_time=backfill_time,\n",
    "    sinks=[redis_sink],  # and/or adls_sink -- TODO can I sepcify both at the same time?\n",
    "    feature_names=agg_feature_names,\n",
    ")\n",
    "\n",
    "client.materialize_features(\n",
    "    settings=settings,\n",
    "    execution_configurations={\"spark.feathr.outputFormat\": \"parquet\"},\n",
    ")\n",
    "\n",
    "client.wait_job_to_finish(timeout_sec=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bef93538-9591-4247-97b6-289d2055b7b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 7. Fetching Feature Values for Online Inference\n",
    "\n",
    "Unknown Issue: Local Spark Feature Gen Job is not working. Job will hang at `RedisOutputUtils.scala:37` when writing to Redis. Still investigating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0c3d5f35-11a3-4644-9992-5860169d8302",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3089.732421875, 6956.0]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_online_features(\n",
    "    feature_table=FEATURE_TABLE_NAME,\n",
    "    key=\"137\",\n",
    "    feature_names=agg_feature_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4d4699ed-42e6-408f-903d-2f799284f4b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'137': [3089.732421875, 6956.0], '265': [4160.6171875, 10000.0]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.multi_get_online_features(\n",
    "    feature_table=\"nycTaxiDemoFeature\",\n",
    "    keys=[\"137\", \"265\"],\n",
    "    feature_names=agg_feature_names,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "nyc_driver_demo",
   "notebookOrigID": 930353059183053,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "c5d1b88564ea095927319e95d120a01ba9530a1c584720276480e541fd6461c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
