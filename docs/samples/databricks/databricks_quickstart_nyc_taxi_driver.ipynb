{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dbutils.widgets.text(\"RESOURCE_PREFIX\", \"\")\n",
        "dbutils.widgets.text(\"REDIS_KEY\", \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "384e5e16-7213-4186-9d04-09d03b155534",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "# Feathr Feature Store on Databricks Demo Notebook\n",
        "\n",
        "This notebook illustrates the use of Feature Store to create a model that predicts NYC Taxi fares. The dataset comes from [here](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page).\n",
        "\n",
        "This notebook is specifically written for Databricks and is relying on some of the Databricks packages such as `dbutils`. The intention here is to provide a \"one click run\" example with minimum configuration. For example:\n",
        "- In this notebook skips feature registry which requires running Azure Purview. \n",
        "- You will need to configure the Redis endpoint to make the online feature query work. \n",
        "\n",
        "The full-fledged notebook can be found from [here](https://github.com/feathr-ai/feathr/blob/main/feathr_project/feathrcli/data/feathr_user_workspace/nyc_driver_demo.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Prerequisite\n",
        "\n",
        "Feathr has native cloud integration. First step is to provision required cloud resources if you want to use Feathr.\n",
        "\n",
        "Follow the [Feathr ARM deployment guide](https://feathr-ai.github.io/feathr/how-to-guides/azure-deployment-arm.html) to run Feathr on Azure. This allows you to quickly get started with automated deployment using Azure Resource Manager template. For more details, please refer [README.md](https://github.com/feathr-ai/feathr#%EF%B8%8F-running-feathr-on-cloud-with-a-few-simple-steps).\n",
        "\n",
        "Additionally, to run this notebook, you'll need to install `feathr` pip package. For local spark, simply run `pip install feathr` on the machine that runs this notebook. To use Databricks or Azure Synapse Analytics, please see dependency management documents:\n",
        "- [Azure Databricks dependency management](https://learn.microsoft.com/en-us/azure/databricks/libraries/)\n",
        "- [Azure Synapse Analytics dependency management](https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-azure-portal-add-libraries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Notebook Steps\n",
        "\n",
        "This tutorial demonstrates the key capabilities of Feathr, including:\n",
        "\n",
        "1. Install Feathr and necessary dependencies.\n",
        "1. Create shareable features with Feathr feature definition configs.\n",
        "1. Create training data using point-in-time correct feature join\n",
        "1. Train and evaluate a prediction model.\n",
        "1. Materialize feature values to the online store.\n",
        "1. Fetch feature value in real-time from online store for online scoring.\n",
        "\n",
        "The overall data flow is as follows:\n",
        "\n",
        "<img src=\"https://github.com/linkedin/feathr/blob/main/docs/images/feature_flow.png?raw=true\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install notebook-scoped library. For details, please see [Azure Databricks dependency management document.](https://learn.microsoft.com/en-us/azure/databricks/libraries/) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "f00b9d0b-94d1-418f-89b9-25bbacb8b068",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "%pip install feathr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check Databricks runtime version\n",
        "spark.conf.get(\"spark.databricks.clusterUsageTags.sparkVersion\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "80223a02-631c-40c8-91b3-a037249ffff9",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta\n",
        "import glob\n",
        "import json\n",
        "from math import sqrt\n",
        "import os\n",
        "import requests\n",
        "from tempfile import NamedTemporaryFile\n",
        "\n",
        "from feathr import (\n",
        "    FeathrClient,\n",
        "    # Feature data types\n",
        "    BOOLEAN, FLOAT, INT32, ValueType,\n",
        "    # Feature data sources\n",
        "    INPUT_CONTEXT, HdfsSource,\n",
        "    # Feature aggregations\n",
        "    TypedKey, WindowAggTransformation,\n",
        "    # Feature types and anchor\n",
        "    DerivedFeature, Feature, FeatureAnchor,\n",
        "    # Materialization\n",
        "    BackfillTime, MaterializationSettings, RedisSink,\n",
        "    # Offline feature computation\n",
        "    FeatureQuery, ObservationSettings,\n",
        ")\n",
        "from feathr.spark_provider.feathr_configurations import SparkExecutionConfiguration\n",
        "import pandas as pd\n",
        "from pyspark.sql import DataFrame\n",
        "import pyspark.sql.functions as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Create Shareable Features with Feathr Feature Definition Configs\n",
        "\n",
        "In this notebook, we define all the necessary resource key values for authentication. These values can also be retrieved by using [Azure Key Vault](https://azure.microsoft.com/en-us/services/key-vault/) cloud key value store.\n",
        "Please refer to [how-to guide documents for granting key-vault access](https://feathr-ai.github.io/feathr/how-to-guides/azure-deployment-arm.html#3-grant-key-vault-and-synapse-access-to-selected-users-optional) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RESOURCE_PREFIX = dbutils.widgets.get(\"RESOURCE_PREFIX\")\n",
        "PROJECT_NAME = \"feathr_getting_started\"\n",
        "ROOT_PATH = \"./\"  # Could be Azure Blob File System path, abfs or wasbs too.\n",
        "\n",
        "REDIS_KEY = dbutils.widgets.get(\"REDIS_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "41d3648a-9bc9-40dc-90da-bc82b21ef9b3",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "Get the required databricks credentials automatically:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "331753d6-1850-47b5-ad97-84b7c01d79d1",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Redis credential\n",
        "os.environ['REDIS_PASSWORD'] = REDIS_KEY\n",
        "\n",
        "# To use a databricks cluster:\n",
        "ctx = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
        "databricks_config = {\n",
        "    'run_name': \"FEATHR_FILL_IN\",\n",
        "    'existing_cluster_id': ctx.tags().get('clusterId').get(),\n",
        "    'libraries': [{'jar': \"FEATHR_FILL_IN\"}],\n",
        "    'spark_jar_task': {\n",
        "        'main_class_name': \"FEATHR_FILL_IN\",\n",
        "        'parameters': [\"FEATHR_FILL_IN\"],\n",
        "    },\n",
        "}\n",
        "os.environ['spark_config__spark_cluster'] = \"databricks\"\n",
        "os.environ['spark_config__databricks__workspace_instance_url'] = \"https://\" + ctx.tags().get('browserHostName').get()\n",
        "os.environ['spark_config__databricks__config_template'] = json.dumps(databricks_config)\n",
        "os.environ['spark_config__databricks__work_dir'] = \"dbfs:/feathr_getting_started\"\n",
        "os.environ['DATABRICKS_WORKSPACE_TOKEN_VALUE'] = ctx.apiToken().get()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "08bc3b7e-bbf5-4e3a-9978-fe1aef8c1aee",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Configurations\n",
        "\n",
        "Feathr uses a yaml file to define configurations. Please refer to [feathr_config.yaml]( https://github.com/linkedin/feathr/blob/main/feathr_project/feathrcli/data/feathr_user_workspace/feathr_config.yaml) for the meaning of each field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "8cd64e3a-376c-48e6-ba41-5197f3591d48",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "yaml_config = f\"\"\"\n",
        "api_version: 1\n",
        "\n",
        "project_config:\n",
        "  project_name: {PROJECT_NAME}\n",
        "  required_environment_variables:\n",
        "    - 'REDIS_PASSWORD'\n",
        "    - 'AZURE_CLIENT_ID'\n",
        "    - 'AZURE_TENANT_ID'\n",
        "    - 'AZURE_CLIENT_SECRET'\n",
        "    \n",
        "feature_registry:\n",
        "  api_endpoint: 'https://{RESOURCE_PREFIX}webapp.azurewebsites.net/api/v1'\n",
        "\n",
        "spark_config:\n",
        "  # Currently support: 'azure_synapse', 'databricks', and 'local'\n",
        "  spark_cluster: 'local'\n",
        "  spark_result_output_parts: '1'\n",
        "\n",
        "offline_store:\n",
        "  wasb:\n",
        "    wasb_enabled: true\n",
        "\n",
        "online_store:\n",
        "  # You can skip this part if you don't have Redis and skip materialization later in this notebook.\n",
        "  redis:\n",
        "    host: '{RESOURCE_PREFIX}redis.redis.cache.windows.net'\n",
        "    port: 6380\n",
        "    ssl_enabled: true\n",
        "\"\"\"\n",
        "\n",
        "tmp = NamedTemporaryFile(mode='w', delete=False)\n",
        "with open(tmp.name, \"w\") as config_file:\n",
        "    config_file.write(yaml_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All the configurations can be overwritten by environment variables with concatenation of `__` for different layers of the config file. For example, `feathr_runtime_location` for databricks config can be overwritten by setting `spark_config__databricks__feathr_runtime_location` environment variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "3fef7f2f-df19-4f53-90a5-ff7999ed983d",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Initialize Feathr Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "9713a2df-c7b2-4562-88b0-b7acce3cc43a",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "client = FeathrClient(config_path=tmp.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "c3b64bda-d42c-4a64-b976-0fb604cf38c5",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### View the NYC taxi fare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "c4ccd7b3-298a-4e5a-8eec-b7e309db393e",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "DATA_FILEPATH = \"wasbs://public@azurefeathrstorage.blob.core.windows.net/sample_data/green_tripdata_2020-04_with_index.csv\"\n",
        "pd.read_csv(\"https://azurefeathrstorage.blob.core.windows.net/public/sample_data/green_tripdata_2020-04_with_index.csv\").head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "7430c942-64e5-4b70-b823-16ce1d1b3cee",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Defining features with Feathr\n",
        "\n",
        "In Feathr, a feature is viewed as a function, mapping a key and timestamp to a feature value. For more details, please see [Feathr Feature Definition Guide](https://github.com/feathr-ai/feathr/blob/main/docs/concepts/feature-definition.md).\n",
        "\n",
        "* The feature key (a.k.a. entity id) identifies the subject of feature, e.g. a user_id or location_id.\n",
        "* The feature name is the aspect of the entity that the feature is indicating, e.g. the age of the user.\n",
        "* The feature value is the actual value of that aspect at a particular time, e.g. the value is 30 at year 2022.\n",
        "\n",
        "Note that, in some cases, a feature could be just a transformation function that has no entity key or timestamp involved, e.g. *the day of week of the request timestamp*.\n",
        "\n",
        "There are two types of features -- anchored features and derivated features:\n",
        "\n",
        "* **Anchored features**: Features that are directly extracted from sources. Could be with or without aggregation. \n",
        "* **Derived features**: Features that are computed on top of other features.\n",
        "\n",
        "#### Define anchored features\n",
        "\n",
        "A feature source is needed for anchored features that describes the raw data in which the feature values are computed from. A source value should be either `INPUT_CONTEXT` (the features that will be extracted from the observation data directly) or `feathr.source.Source` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TIMESTAMP_COL = \"lpep_dropoff_datetime\"\n",
        "TIMESTAMP_FORMAT = \"yyyy-MM-dd HH:mm:ss\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We define f_trip_distance and f_trip_time_duration features separately\n",
        "# so that we can reuse them later for the derived features.\n",
        "f_trip_distance = Feature(\n",
        "    name=\"f_trip_distance\",\n",
        "    feature_type=FLOAT,\n",
        "    transform=\"trip_distance\",\n",
        ")\n",
        "f_trip_time_duration = Feature(\n",
        "    name=\"f_trip_time_duration\",\n",
        "    feature_type=FLOAT,\n",
        "    transform=\"cast_float((to_unix_timestamp(lpep_dropoff_datetime) - to_unix_timestamp(lpep_pickup_datetime)) / 60)\",\n",
        ")\n",
        "\n",
        "features = [\n",
        "    f_trip_distance,\n",
        "    f_trip_time_duration,\n",
        "    Feature(\n",
        "        name=\"f_is_long_trip_distance\",\n",
        "        feature_type=BOOLEAN,\n",
        "        transform=\"trip_distance > 30.0\",\n",
        "    ),\n",
        "    Feature(\n",
        "        name=\"f_day_of_week\",\n",
        "        feature_type=INT32,\n",
        "        transform=\"dayofweek(lpep_dropoff_datetime)\",\n",
        "    ),\n",
        "    Feature(\n",
        "        name=\"f_day_of_month\",\n",
        "        feature_type=INT32,\n",
        "        transform=\"dayofmonth(lpep_dropoff_datetime)\",\n",
        "    ),\n",
        "    Feature(\n",
        "        name=\"f_hour_of_day\",\n",
        "        feature_type=INT32,\n",
        "        transform=\"hour(lpep_dropoff_datetime)\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "# After you have defined features, bring them together to build the anchor to the source.\n",
        "feature_anchor = FeatureAnchor(\n",
        "    name=\"feature_anchor\",\n",
        "    source=INPUT_CONTEXT,  # Pass through source, i.e. observation data.\n",
        "    features=features,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "728d2d5f-c11f-4941-bdc5-48507f5749f1",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "We can define the source with a preprocessing python function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "3cc59a0e-a41b-480e-a84e-ca5443d63143",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "def preprocessing(df: DataFrame) -> DataFrame:\n",
        "    import pyspark.sql.functions as F\n",
        "    df = df.withColumn(\"fare_amount_cents\", (F.col(\"fare_amount\") * 100.0).cast(\"float\"))\n",
        "    return df\n",
        "\n",
        "batch_source = HdfsSource(\n",
        "    name=\"nycTaxiBatchSource\",\n",
        "    path=DATA_FILEPATH,\n",
        "    event_timestamp_column=TIMESTAMP_COL,\n",
        "    preprocessing=preprocessing,\n",
        "    timestamp_format=TIMESTAMP_FORMAT,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "46f863c4-bb81-434a-a448-6b585031a221",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "For the features with aggregation, the supported functions are as follows:\n",
        "\n",
        "| Aggregation Function | Input Type | Description |\n",
        "| --- | --- | --- |\n",
        "|SUM, COUNT, MAX, MIN, AVG\t|Numeric|Applies the the numerical operation on the numeric inputs. |\n",
        "|MAX_POOLING, MIN_POOLING, AVG_POOLING\t| Numeric Vector | Applies the max/min/avg operation on a per entry bassis for a given a collection of numbers.|\n",
        "|LATEST| Any |Returns the latest not-null values from within the defined time window |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "a373ecbe-a040-4cd3-9d87-0d5f4c5ba553",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "agg_key = TypedKey(\n",
        "    key_column=\"DOLocationID\",\n",
        "    key_column_type=ValueType.INT32,\n",
        "    description=\"location id in NYC\",\n",
        "    full_name=\"nyc_taxi.location_id\",\n",
        ")\n",
        "\n",
        "agg_window = \"90d\"\n",
        "\n",
        "# Anchored features with aggregations\n",
        "agg_features = [\n",
        "    Feature(\n",
        "        name=\"f_location_avg_fare\",\n",
        "        key=agg_key,\n",
        "        feature_type=FLOAT,\n",
        "        transform=WindowAggTransformation(\n",
        "            agg_expr=\"fare_amount_cents\",\n",
        "            agg_func=\"AVG\",\n",
        "            window=agg_window,\n",
        "        ),\n",
        "    ),\n",
        "    Feature(\n",
        "        name=\"f_location_max_fare\",\n",
        "        key=agg_key,\n",
        "        feature_type=FLOAT,\n",
        "        transform=WindowAggTransformation(\n",
        "            agg_expr=\"fare_amount_cents\",\n",
        "            agg_func=\"MAX\",\n",
        "            window=agg_window,\n",
        "        ),\n",
        "    ),\n",
        "]\n",
        "\n",
        "agg_feature_anchor = FeatureAnchor(\n",
        "    name=\"agg_feature_anchor\",\n",
        "    source=batch_source,  # External data source for feature. Typically a data table.\n",
        "    features=agg_features,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "149f85e2-fa3c-4895-b0c5-de5543ca9b6d",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "#### Define derived features\n",
        "\n",
        "We also define a derived feature, `f_trip_time_distance`, from the anchored features `f_trip_distance` and `f_trip_time_duration` as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "05633bc3-9118-449b-9562-45fc437576c2",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "derived_features = [\n",
        "    DerivedFeature(\n",
        "        name=\"f_trip_time_distance\",\n",
        "        feature_type=FLOAT,\n",
        "        input_features=[\n",
        "            f_trip_distance,\n",
        "            f_trip_time_duration,\n",
        "        ],\n",
        "        transform=\"f_trip_distance / f_trip_time_duration\",\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "ad102c45-586d-468c-85f0-9454401ef10b",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Build features\n",
        "\n",
        "Finally, we build the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "91bb5ebb-87e4-470b-b8eb-1c89b351740e",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "client.build_features(\n",
        "    anchor_list=[feature_anchor, agg_feature_anchor],\n",
        "    derived_feature_list=derived_features,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "632d5f46-f9e2-41a8-aab7-34f75206e2aa",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "## 3. Create Training Data Using Point-in-Time Correct Feature Join\n",
        "\n",
        "After the feature producers have defined the features (as described in the Feature Definition part), the feature consumers may want to consume those features. Feature consumers will use observation data to query from different feature tables using Feature Query.\n",
        "\n",
        "To create a training dataset using Feathr, one needs to provide a feature join configuration file to specify\n",
        "what features and how these features should be joined to the observation data. \n",
        "\n",
        "To learn more on this topic, please refer to [Point-in-time Correctness](https://github.com/linkedin/feathr/blob/main/docs/concepts/point-in-time-join.md)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "e438e6d8-162e-4aa3-b3b3-9d1f3b0d2b7f",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "DATA_FORMAT = \"parquet\"\n",
        "offline_features_path = f\"dbfs:/feathr_output/{PROJECT_NAME}/features.{DATA_FORMAT}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "feature_names = [feature.name for feature in features + derived_features]\n",
        "print(\"Features that will be extracted directly from the observation data:\", feature_names)\n",
        "\n",
        "agg_feature_names = [feature.name for feature in agg_features]\n",
        "print(\"Features that will be extracted from the offline source data:\", agg_feature_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Features that we want to request. Can use a subset of features\n",
        "query = FeatureQuery(\n",
        "    feature_list=feature_names + agg_feature_names,\n",
        "    key=agg_key,\n",
        ")\n",
        "settings = ObservationSettings(\n",
        "    observation_path=DATA_FILEPATH,\n",
        "    event_timestamp_column=TIMESTAMP_COL,\n",
        "    timestamp_format=TIMESTAMP_FORMAT,\n",
        ")\n",
        "client.get_offline_features(\n",
        "    observation_settings=settings,\n",
        "    feature_query=query,\n",
        "    # TODO - this doesn't work. It keeps storing as \"avro\" files\n",
        "    execution_configurations=SparkExecutionConfiguration({\n",
        "        \"spark.feathr.inputFormat\": DATA_FORMAT,\n",
        "        \"spark.feathr.outputFormat\": DATA_FORMAT,\n",
        "    }),\n",
        "    output_path=offline_features_path,\n",
        ")\n",
        "\n",
        "client.wait_job_to_finish(timeout_sec=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "dcbf17fc-7f79-4a65-a3af-9cffbd0b5d1f",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "## 4. Train and Evaluate a Prediction Model\n",
        "\n",
        "After generating all the features, we train and evaluate a machine learning model to predict the NYC taxi fare prediction. In this example, we use Spark MLlib's [GBTRegressor](https://spark.apache.org/docs/latest/ml-classification-regression.html#gradient-boosted-tree-regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "84745f36-5bac-49c0-903b-38828b923c7c",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import GBTRegressor\n",
        "\n",
        "# TODO you may need to add {\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.3.0\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "5a226026-1c7b-48db-8f91-88d5c2ddf023",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Load Train and Test Data from the Offline Feature Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "3b924c66-8634-42fe-90f3-c844487d3f75",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "df = spark.read.format(DATA_FORMAT).load(offline_features_path)\n",
        "df.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train / test split\n",
        "train_df, test_df = (\n",
        "    df\n",
        "    .withColumn(\"label\", F.col(\"fare_amount\").cast(\"double\"))\n",
        "    .where(F.col(\"f_trip_time_duration\") > 0)\n",
        "    .fillna(0)\n",
        "    .randomSplit([0.8, 0.2])\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "6a3e2ab1-5c66-4d27-a737-c5e2af03b1dd",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Build a ML Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a feature vector column for SparkML\n",
        "vector_assembler = VectorAssembler(\n",
        "    inputCols=[x for x in df.columns if x in feature_names + agg_feature_names],\n",
        "    outputCol=\"features\",\n",
        ")\n",
        "\n",
        "# Define a model\n",
        "gbt = GBTRegressor(\n",
        "    featuresCol=\"features\",\n",
        "    maxIter=100,\n",
        "    maxDepth=5,\n",
        "    maxBins=16,\n",
        ")\n",
        "\n",
        "# Create a ML pipeline\n",
        "ml_pipeline = Pipeline(stages=[\n",
        "    vector_assembler,\n",
        "    gbt,\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "bef93538-9591-4247-97b6-289d2055b7b1",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Train and Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "0c3d5f35-11a3-4644-9992-5860169d8302",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Train a model\n",
        "model = ml_pipeline.fit(train_df)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.transform(test_df)\n",
        "\n",
        "# Evaluate\n",
        "evaluator = RegressionEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"rmse\",\n",
        ")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "\n",
        "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Materialize Feature Values to the Online Store\n",
        "\n",
        "While we computed feature values on-the-fly at request time via Feathr, we can pre-compute the feature values and materialize them to offline or online storages such as Redis.\n",
        "\n",
        "Note, only the features anchored to offline data source can be materialized, in our case `agg_features`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "4d4699ed-42e6-408f-903d-2f799284f4b6",
          "showTitle": false,
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "FEATURE_TABLE_NAME = \"nycTaxiDemoFeature\"\n",
        "\n",
        "# Time range to materialize -- TODO how to properly set this?\n",
        "backfill_time = BackfillTime(\n",
        "    start=datetime(2020, 5, 20),\n",
        "    end=datetime(2020, 5, 20),\n",
        "    step=timedelta(days=1),\n",
        ")\n",
        "\n",
        "# Destinations\n",
        "redis_sink = RedisSink(table_name=FEATURE_TABLE_NAME)\n",
        "\n",
        "settings = MaterializationSettings(\n",
        "    name=FEATURE_TABLE_NAME + \".job\",  # job name -- TODO if not important, automate this, e.g. redis_sink.table_name + \".job\"\n",
        "    backfill_time=backfill_time,\n",
        "    sinks=[redis_sink],\n",
        "    feature_names=agg_feature_names,\n",
        ")\n",
        "\n",
        "client.materialize_features(\n",
        "    settings=settings,\n",
        "    execution_configurations={\"spark.feathr.outputFormat\": \"parquet\"},\n",
        ")\n",
        "\n",
        "client.wait_job_to_finish(timeout_sec=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Fetching Feature Values for Online Inference\n",
        "\n",
        "Unknown Issue: Local Spark Feature Gen Job is not working. Job will hang at `RedisOutputUtils.scala:37` when writing to Redis. Still investigating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client.get_online_features(\n",
        "    feature_table=FEATURE_TABLE_NAME,\n",
        "    key=\"137\",\n",
        "    feature_names=agg_feature_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client.multi_get_online_features(\n",
        "    feature_table=FEATURE_TABLE_NAME,\n",
        "    keys=[\"137\", \"265\"],\n",
        "    feature_names=agg_feature_names,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "nyc_driver_demo",
      "notebookOrigID": 930353059183053,
      "widgets": {}
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 ('logistics')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "6d25d3d1f1809ed0384c3d8e0cd4f1df57fe7bb936ead67f035c6ff1494f4e23"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
